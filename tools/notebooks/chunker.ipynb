{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fubot_root': '../../', 'output_root': '../Samples/output', 'output_name': 'dataset_6p', 'split': {'categories': ['train', 'val', 'test'], 'ratios': [0.65, 0.25, 0.1], 'min_chunk_size': 30, 'max_chunk_size': 300, 'seed': -1}, 'training': {'input_features': [['Head', '', ['pos[xyz]_w', 'qrot[wxyz]_w']], ['Hips', '', ['pos[xyz]_w', 'qrot[wxyz]_w']], ['LeftHand', '', ['pos[xyz]_w', 'qrot[wxyz]_w']], ['RightHand', '', ['pos[xyz]_w', 'qrot[wxyz]_w']], ['LeftToes', '', ['pos[xyz]_w', 'qrot[wxyz]_w']], ['RightToes', '', ['pos[xyz]_w', 'qrot[wxyz]_w']]], 'output_features': [['Hips', '', ['pos[xyz]_l', 'qrot[wxyz]_l']], ['Spine', '', ['qrot[wxyz]_l']], ['Chest', '', ['qrot[wxyz]_l']], ['UpperChest', '', ['qrot[wxyz]_l']], ['Neck', '', ['qrot[wxyz]_l']], ['Head', '', ['qrot[wxyz]_l']], ['LeftShoulder', '', ['qrot[wxyz]_l']], ['LeftUpperArm', '', ['qrot[wxyz]_l']], ['LeftLowerArm', '', ['qrot[wxyz]_l']], ['LeftHand', '', ['qrot[wxyz]_l']], ['RightShoulder', '', ['qrot[wxyz]_l']], ['RightUpperArm', '', ['qrot[wxyz]_l']], ['RightLowerArm', '', ['qrot[wxyz]_l']], ['RightHand', '', ['qrot[wxyz]_l']], ['LeftUpperLeg', '', ['qrot[wxyz]_l']], ['LeftLowerLeg', '', ['qrot[wxyz]_l']], ['LeftFoot', '', ['qrot[wxyz]_l']], ['LeftToes', '', ['qrot[wxyz]_l']], ['RightUpperLeg', '', ['qrot[wxyz]_l']], ['RightLowerLeg', '', ['qrot[wxyz]_l']], ['RightFoot', '', ['qrot[wxyz]_l']], ['RightToes', '', ['qrot[wxyz]_l']]]}}\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "import re\r\n",
    "\r\n",
    "class fubot_config():\r\n",
    "    class feat_group():\r\n",
    "        class feat_type():\r\n",
    "            lut_C = {'w': 1, 'x':2, 'y':3, 'z':4}\r\n",
    "            lut_C_inv = list(lut_C.keys())\r\n",
    "            lut_S = {'l': 1, 'w':2}\r\n",
    "            lut_T = {'pos': 1, 'qrot':2, 'rot':3}\r\n",
    "\r\n",
    "            def __init__(self, S, T, C):              \r\n",
    "                self.S = self.lut_S[S]\r\n",
    "                self.T = self.lut_T[T]\r\n",
    "                self.C = self.lut_C[C]\r\n",
    "                self.C_id = self.C - (1 if self.T == 2 else 2)\r\n",
    "                self.valid = (self.S * self.T * self.C) > 0\r\n",
    "\r\n",
    "            def getValue(self, v_local, v_world):\r\n",
    "                return v_local[self.C_id] if self.S == 1 else v_world[self.C_id]\r\n",
    "\r\n",
    "            def getPostFix(self):\r\n",
    "                pt = 'p' if self.T == 1 else 'r'\r\n",
    "                pc = self.lut_C_inv[self.C - 1]\r\n",
    "                \r\n",
    "                return f'_{pt}{pc}'\r\n",
    "\r\n",
    "        def __init__(self, feature_config):\r\n",
    "            self.name = None\r\n",
    "            self.bone_name = None\r\n",
    "            self.elements = []\r\n",
    "            self.size = 0\r\n",
    "            self.rgx_el = re.compile(r'(.+)\\[(.+)\\]_(.)')\r\n",
    "            self.__parse__(feature_config)\r\n",
    "\r\n",
    "        def __parse__(self, feature_config):\r\n",
    "            self.name = feature_config[0]\r\n",
    "            self.bone_name = feature_config[1] if len(feature_config[1]) > 0 else self.name\r\n",
    "\r\n",
    "            #Feature Elements\r\n",
    "            for f_el in feature_config[2]:\r\n",
    "                m = self.rgx_el.match(f_el)\r\n",
    "                g = m.groups()\r\n",
    "                if len(g) is not 3:\r\n",
    "                    print(f'Invalid feature element found. ({self.name} >> {f_el})')\r\n",
    "                    continue\r\n",
    "\r\n",
    "                for comp in g[1]:\r\n",
    "                    ft = self.feat_type(g[2], g[0], comp)\r\n",
    "                    if not ft.valid:\r\n",
    "                        print(f'Invalid feature element found. ({self.name} >> {f_el} [{g[2]}, {g[0]}, {comp}])')\r\n",
    "                        continue\r\n",
    "\r\n",
    "                    self.elements.append(ft)\r\n",
    "            \r\n",
    "            self.size = len(self.elements)\r\n",
    "\r\n",
    "    def __init__(self, config_path):\r\n",
    "        self.features_output:self.feat_group = []\r\n",
    "        self.features_output_header = []\r\n",
    "        self.features_output_size = 0\r\n",
    "\r\n",
    "        self.features_input:self.feat_group = []\r\n",
    "        self.features_input_header = []\r\n",
    "        self.features_input_size = 0\r\n",
    "        self.params = None\r\n",
    "\r\n",
    "        self.__loadConfig__(config_path)\r\n",
    "\r\n",
    "    def __loadConfig__(self, config_path):\r\n",
    "        with open(config_path) as f:\r\n",
    "            self.params = json.loads(f.read())\r\n",
    "\r\n",
    "        #Training Features\r\n",
    "        #input\r\n",
    "        features = self.params['training']['input_features']\r\n",
    "        if features is not None:\r\n",
    "            for feat in features:\r\n",
    "                fg = self.feat_group(feat)\r\n",
    "                self.features_input.append(fg)\r\n",
    "                self.features_input_size += fg.size\r\n",
    "\r\n",
    "                #Create Header\r\n",
    "                for ft in fg.elements:\r\n",
    "                    self.features_input_header.append(f'{fg.bone_name}{ft.getPostFix()}')\r\n",
    "\r\n",
    "                \r\n",
    "\r\n",
    "        #output\r\n",
    "        features = self.params['training']['output_features']\r\n",
    "        if features is not None:\r\n",
    "            for feat in features:\r\n",
    "                fg = self.feat_group(feat)\r\n",
    "                self.features_output.append(fg)\r\n",
    "                self.features_output_size += fg.size\r\n",
    "\r\n",
    "                #Create Header\r\n",
    "                for ft in fg.elements:\r\n",
    "                    self.features_output_header.append(f'{fg.bone_name}{ft.getPostFix()}')\r\n",
    "\r\n",
    "config = fubot_config('../samples/generator_config.json')\r\n",
    "print(config.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import random\r\n",
    "import datetime\r\n",
    "\r\n",
    "def calculate_frames_and_chunks(num_frames, split, minC, maxC):\r\n",
    "    f = int(num_frames * split)\r\n",
    "    c = int(f / maxC)\r\n",
    "    if f - (c * maxC) > minC:\r\n",
    "        c += 1\r\n",
    "\r\n",
    "    return f, c\r\n",
    "\r\n",
    "def chunk_frames(num_frames, split_params):\r\n",
    "    #read params\r\n",
    "    min_chunk_size = split_params['min_chunk_size']\r\n",
    "    max_chunk_size = split_params['max_chunk_size']\r\n",
    "    ratios = split_params['split_ratios']\r\n",
    "    categories = split_params['split_categories']\r\n",
    "    num_categories = len(categories)\r\n",
    "\r\n",
    "    sp_frames = [0] * num_categories\r\n",
    "    sp_chunks = [0] * num_categories\r\n",
    "    tot_frames = 0\r\n",
    "    tot_chunks = 0\r\n",
    "    rnd_distribution = list()\r\n",
    "    for cat_id in range(num_categories):\r\n",
    "        sp_frames[cat_id], sp_chunks[cat_id] = calculate_frames_and_chunks(num_frames, ratios[cat_id], min_chunk_size, max_chunk_size)\r\n",
    "        tot_frames += sp_frames[cat_id]\r\n",
    "        tot_chunks += sp_chunks[cat_id]\r\n",
    "\r\n",
    "        rnd_distribution.extend([cat_id] * sp_chunks[cat_id])\r\n",
    "\r\n",
    "    #store params\r\n",
    "    split_params['sp_frames'] = sp_frames.copy()\r\n",
    "    split_params['sp_chunks'] = sp_chunks\r\n",
    "    split_params['tot_frames'] = tot_frames\r\n",
    "    split_params['tot_chunks'] = tot_chunks\r\n",
    "\r\n",
    "    random.shuffle(rnd_distribution)\r\n",
    "\r\n",
    "    split_order = list()\r\n",
    "    for rnd_id in rnd_distribution:\r\n",
    "        chunkSize = max_chunk_size\r\n",
    "        if sp_frames[rnd_id] < chunkSize:\r\n",
    "            chunkSize = sp_frames[rnd_id]\r\n",
    "        sp_frames[rnd_id] -= chunkSize\r\n",
    "        \r\n",
    "        split_order.append(f'{categories[rnd_id]}_{chunkSize}')\r\n",
    "\r\n",
    "    return split_order, split_params\r\n",
    "\r\n",
    "def split_source_file(split_params):\r\n",
    "    #read params\r\n",
    "    sample_name = split_params['sample_name']\r\n",
    "    source_file = split_params['sample_source_path']\r\n",
    "    output_root = split_params['sample_output']    \r\n",
    "\r\n",
    "    with open(source_file) as f_in:\r\n",
    "        #collect hierarchy\r\n",
    "        hierarchy_str = ''\r\n",
    "\r\n",
    "        while True:\r\n",
    "            line = f_in.readline()\r\n",
    "            hierarchy_str += line\r\n",
    "            if line.startswith('MOTION'):\r\n",
    "                break\r\n",
    "\r\n",
    "        #split bvh motion\r\n",
    "        source_num_frames = int(f_in.readline().split('    ')[1])\r\n",
    "        split_key, split_params = chunk_frames(source_num_frames, split_params)\r\n",
    "        frame_time = f_in.readline()\r\n",
    "\r\n",
    "        split_cat_counter = {}\r\n",
    "        for split_id, split_entry in enumerate(split_key):\r\n",
    "            split_cat = split_entry.split('_')[0]\r\n",
    "            num_frames = int(split_entry.split('_')[1])\r\n",
    "\r\n",
    "            if split_cat in split_cat_counter:\r\n",
    "                split_cat_counter[split_cat] += 1\r\n",
    "            else:\r\n",
    "                split_cat_counter[split_cat] = 0\r\n",
    "            \r\n",
    "            filepath = os.path.join(output_root, split_cat, 'bvh')\r\n",
    "            os.makedirs(filepath, exist_ok=True)\r\n",
    "            filepath = os.path.join(filepath, f'{sample_name}_{split_cat_counter[split_cat]}_b.bvh')\r\n",
    "            #filepath = os.path.join(filepath, f'{sample_name}_{split_id}_b.bvh')\r\n",
    "\r\n",
    "            with open(filepath, 'w') as f_out:\r\n",
    "                f_out.write(hierarchy_str)\r\n",
    "                f_out.write(f'Frames:    {num_frames}\\n')\r\n",
    "                f_out.write(frame_time)\r\n",
    "\r\n",
    "                #Frames\r\n",
    "                for _ in range(num_frames):\r\n",
    "                    f_out.write(f_in.readline())\r\n",
    "\r\n",
    "    return split_params\r\n",
    "\r\n",
    "def create_sample_metafile(split_params, gds_meta):\r\n",
    "    game_meta_root = os.path.join(split_params['sample_output'], 'meta')\r\n",
    "    os.makedirs(game_meta_root, exist_ok=True)\r\n",
    "\r\n",
    "    sample_data = {\r\n",
    "        'name':split_params['sample_name'],\r\n",
    "        'game_id':split_params['game_id'],\r\n",
    "        'sample_id':split_params['sample_id'],\r\n",
    "        'source_file':split_params['source_file_path'],\r\n",
    "        'min_chunk_size':split_params['min_chunk_size'],\r\n",
    "        'max_chunk_size':split_params['max_chunk_size'],\r\n",
    "        'split_categories':split_params['split_categories'],\r\n",
    "        'split_ratios':split_params['split_ratios'],\r\n",
    "        'num_chunks':{\r\n",
    "            'total':split_params['tot_chunks']\r\n",
    "        },\r\n",
    "        'num_frames':{\r\n",
    "            'total':split_params['tot_frames']\r\n",
    "        }\r\n",
    "    }\r\n",
    "\r\n",
    "    #update game ds meta\r\n",
    "    gds_meta['sample_ids'].append(sample_data['name'])\r\n",
    "    gds_meta['total_chunks']['total'] += split_params['tot_chunks']\r\n",
    "    gds_meta['total_frames']['total'] += split_params['tot_frames']\r\n",
    "\r\n",
    "    for cat_id, cat_name in enumerate(split_params['split_categories']):   \r\n",
    "        sample_data['num_chunks'][cat_name] = split_params['sp_chunks'][cat_id]\r\n",
    "        sample_data['num_frames'][cat_name] = split_params['sp_frames'][cat_id]\r\n",
    "\r\n",
    "        #gds meta\r\n",
    "        gds_meta['total_chunks'][cat_name] += split_params['sp_chunks'][cat_id]\r\n",
    "        gds_meta['total_frames'][cat_name] += split_params['sp_frames'][cat_id]\r\n",
    "\r\n",
    "    metafile_path = os.path.join(game_meta_root, split_params['sample_name'] + '.json')\r\n",
    "    with open(metafile_path, 'w') as f:\r\n",
    "        json.dump(sample_data, f, indent=2)\r\n",
    "\r\n",
    "def chunk_dataset(config:fubot_config):\r\n",
    "    #Random Seed\r\n",
    "    rnd_seed = config.params['split']['seed']\r\n",
    "    if rnd_seed < 0:\r\n",
    "        rnd_seed = datetime.now()\r\n",
    "    \r\n",
    "    random.seed(rnd_seed)\r\n",
    "\r\n",
    "\r\n",
    "    #Split Params\r\n",
    "    sp = {\r\n",
    "        'dataset_root': config.params['fubot_root'],\r\n",
    "        'output_root': os.path.join(config.params['output_root'],config.params['output_name']),\r\n",
    "        'split_categories':config.params['split']['categories'],\r\n",
    "        'split_ratios':config.params['split']['ratios'],\r\n",
    "        'min_chunk_size':config.params['split']['min_chunk_size'],\r\n",
    "        'max_chunk_size':config.params['split']['max_chunk_size']\r\n",
    "    }\r\n",
    "\r\n",
    "    dataset_root = sp['dataset_root']\r\n",
    "    meta_root_path = os.path.join(dataset_root, 'meta')\r\n",
    "    game_ids = next(os.walk(meta_root_path))[1]\r\n",
    "\r\n",
    "    def summary_dict():\r\n",
    "        d = {'total':0}\r\n",
    "        for c in sp['split_categories']:\r\n",
    "            d[c] = 0\r\n",
    "\r\n",
    "        return d\r\n",
    "\r\n",
    "    def append_dicts(a, b):\r\n",
    "        a['total'] += b['total']\r\n",
    "        for c in sp['split_categories']:\r\n",
    "            a[c] += b[c]\r\n",
    "\r\n",
    "    ds_meta = {\r\n",
    "        'name':config.params['output_name'],\r\n",
    "        'sample_rate':30, #fixed for now\r\n",
    "        'length':-1,\r\n",
    "        'split_categories':sp['split_categories'],\r\n",
    "        'split_ratios':sp['split_ratios'],\r\n",
    "        'split_seed':rnd_seed,\r\n",
    "        'min_chunk_size':sp['min_chunk_size'],\r\n",
    "        'max_chunk_size':sp['max_chunk_size'],\r\n",
    "        'total_chunks': summary_dict(),\r\n",
    "        'total_frames': summary_dict(),\r\n",
    "        'game_ids':list()\r\n",
    "    }\r\n",
    "\r\n",
    "    for game_id in game_ids:\r\n",
    "        meta_game_root_path = os.path.join(meta_root_path, game_id)\r\n",
    "        game_samples = next(os.walk(meta_game_root_path))[2]\r\n",
    "\r\n",
    "        #update DS META\r\n",
    "        game_ds_meta = {\r\n",
    "            'name':game_id,\r\n",
    "            'sample_ids': list(),\r\n",
    "            'total_chunks':summary_dict(),\r\n",
    "            'total_frames':summary_dict(),\r\n",
    "        }\r\n",
    "        ds_meta['game_ids'].append(game_ds_meta)\r\n",
    "\r\n",
    "        for game_sample in game_samples:\r\n",
    "            meta_sample_path = os.path.join(meta_game_root_path, game_sample)\r\n",
    "            with open(meta_sample_path) as f:\r\n",
    "                sample_json = json.loads(f.read())\r\n",
    "\r\n",
    "            sp['sample_source_path'] = os.path.join(dataset_root, sample_json['source_file'])\r\n",
    "            sp['source_file_path'] = sample_json['source_file']\r\n",
    "            sample_name = os.path.basename(sample_json['source_file'])\r\n",
    "            sample_name = os.path.splitext(sample_name)[0]\r\n",
    "            sp['sample_name'] = sample_name\r\n",
    "            sp['game_id'] = sample_json['game_id'].lower()\r\n",
    "            sp['sample_id'] = sample_json['sample_id']\r\n",
    "            sp['sample_output'] = os.path.join(sp['output_root'], sp['game_id'])\r\n",
    "\r\n",
    "            sp = split_source_file(sp)\r\n",
    "            #Generate Metafile\r\n",
    "            create_sample_metafile(sp, game_ds_meta)\r\n",
    "\r\n",
    "        append_dicts(ds_meta['total_chunks'], game_ds_meta['total_chunks'])\r\n",
    "        append_dicts(ds_meta['total_frames'], game_ds_meta['total_frames'])\r\n",
    "\r\n",
    "    ds_meta['length'] = str(datetime.timedelta(seconds=ds_meta['total_frames']['total']/ds_meta['sample_rate']))\r\n",
    "\r\n",
    "    #Save DS Meta\r\n",
    "    with open(os.path.join(sp['output_root'], config.params['output_name'] + '_meta.json'), 'w') as f:\r\n",
    "        json.dump(ds_meta, f, indent=2)\r\n",
    "\r\n",
    "config = fubot_config('../samples/generator_config.json')\r\n",
    "chunk_dataset(config)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python376jvsc74a57bd00600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}